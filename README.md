## Feature examples

```
position, tool, category, severity, redundancy_level, category_frequency, tool_fp_rate, positive?
foo.c:47, cppcheck, buffer overflow, critical, 1, 10, 0.3,  true
```

It is also possible that we would benefit of a binary feature for each of the
static analyzers, where it is true for the presence of the same bug in the
analyzer and false otherwise

## Notes on Juliet 1.2 test cases

* There are regular expressions to identify GOOD and BAD functions
* There are makefiles for some CWEs, where a binary is built on for files
  without Windows dependencies
* Some test cases are bad only test cases. They should not be used if you want
  to determine the number of false positives generated by a tool I do believe
  they may be useful for this experiment). These cases are listed in appendix D
  of Juliet User guide under `juliet/doc`
* Accidental flaws (i.e. non-intentional bugs in Juliet) may exist, and they
  should be ignored.

## how to run

To run this experiment, you need the following software installed

* python >= 3.6
* cppcheck
* flawfinder
* frama-c
* scan-build (clang-analyzer)

Just run  `make` to download and prepare the test suite and start running the
analyzers.

The results will be stored under the `reports` directory.

# Log

## Collecting static analysis reports:

Static analysis reports were collected by runnings 4 static analysis tools in
Juliet 1.2:

* Frama-C
* flawfinder
* cppcheck
* scan-build (clang analyzer)

The parameters used to run each script can be seen in the `run_analyses.sh`
script.

We ran the analyzers in a subset of Juliet, removing the testcases calling
types or functions that were specific to Windows systems. A complete list
of the files analyzed is generated with the `bootstrap.sh` script.

It is worth mentioning that for Frama-C, we also had to ignore the C++ test
cases, since this tool can only analyze C programs.

**TODO: GENERATE TABLES WITH NUMBERS OF TESTCASES FOR EACH CWE AND NUMBERS OF
TESTCASES FOR EACH CWE USED IN THE EXPERIMENT.**

## Preprocessing the reports to generate the trainning set

**TODO: talk about unrelated flaws (section 8.3 in Juliet documentation)**

After generating the reports, we need to pre-process them before we are able to
use them to train our model. We need to

* Label warnings as true/false positives
* Remove warnings not related to the CWE being tested generated for each test
  case (this is needed because accidental flaws may exist)
* Collect potential features for the training set

### Labelling true/false positives

We want to use the regular expressions provided by Juliet documentation to
match the bad functions. We also need to map the warning messages from each
tool with the CWE in question. after this:

* Warnings related to the CWE, in bad functions, will be considered true positives
* Warnings related to the CWE, in good functions, will be considered false positives
* All the other warnings will be ignored and will not be used in our trainnings set

**TODO: GENERATE TABLE WITH MAPPINGS WARNING-MESSAGE=>CWE FOR EACH TOOL**

**TODO: GENERATE TABLES OR CHARTS WITH DATA CONTAINING NUMBERS OF WARNINGS
GENERATED BY EACH TOOL FOR EACH TEST CASE. HOW MANY WERE USED? HOW MANY WERE
IGNORED? (FOR EACH CASE)**

To label a warning, first we need to find in which function it belongs, the following
code snippet might help (the 1st line gives the function name and starting
line, the second line gives the function last line this must go in a script):

```
ctags -x --c-kinds=f filename.c
awk 'NR > first && /^}$/ { print NR; exit }' first=$FIRST_LINE filename.c
```
with this data, we should start generating a CSV file, with information about the
tool, file, line, label

### Adding features

For each of the desired features, a new entry is added to the CSV file
generated in the step above.

## Generating the ranking model
