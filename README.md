## Feature examples

```
position, tool, category, severity, redundancy_level, category_frequency, tool_fp_rate, positive?
foo.c:47, cppcheck, buffer overflow, critical, 1, 10, 0.3,  true
```

It is also possible that we would benefit of a binary feature for each of the
static analyzers, where it is true for the presence of the same bug in the
analyzer and false otherwise

## Notes on Juliet 1.2 test cases

* There are regular expressions to identify GOOD and BAD functions
* There are makefiles for some CWEs, where a binary is built on for files
  without Windows dependencies
* Some test cases are bad only test cases. They should not be used if you want
  to determine the number of false positives generated by a tool I do believe
  they may be useful for this experiment). These cases are listed in appendix D
  of Juliet User guide under `juliet/doc`
* Accidental flaws (i.e. non-intentional bugs in Juliet) may exist, and they
  should be ignored.

## how to run

To run this experiment, you need the following software installed

* python >= 3.6
* cppcheck
* flawfinder
* frama-c
* scan-build (clang-analyzer)

Just run  `make` to download and prepare the test suite and start running the
analyzers.

The results will be stored under the `reports` directory.
